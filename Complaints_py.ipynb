{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of complaints.py",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWkT2VzPTvSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# # Multiclass Classification For User Complaints in Banking\n",
        "# \n",
        "# ## Introduction\n",
        "# This is an NLP-based problem solving approach for the dataset available at http://www.cs.toronto.edu/~complingweb/data/karaOne/karaOne.html\n",
        "#domain - automotive \n",
        "\n",
        "import nltk\n",
        "import pickle\n",
        "import gensim\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from stop_words import get_stop_words\n",
        "import re, sys, math, string\n",
        "import calendar as cal\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "import logging\n",
        "from gensim.models import word2vec\n",
        "\n",
        "#from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from  sklearn.calibration import CalibratedClassifierCV\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
        "from keras.layers.core import Reshape, Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "import altair as alt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from numpy import array\n",
        "\n",
        "main_df = pd.read_csv('data/Consumer_Complaints.csv')\n",
        "\n",
        "\n",
        "stplist = ['title', 'body', 'xxxx']\n",
        "english_stopwords = get_stop_words(language='english')\n",
        "english_stopwords += stplist\n",
        "english_stopwords = list(set(english_stopwords))\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"\n",
        "    Function that determines the the Part-of-speech (POS) tag.\n",
        "    Acts as input to lemmatizer. Result is of the form: [('complaint', 'NN'), ... ]\n",
        "    \"\"\"\n",
        "    if word.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif word.startswith('V'):\n",
        "        return wn.VERB\n",
        "    elif word.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif word.startswith('R'):\n",
        "        return wn.ADV\n",
        "    else:\n",
        "        return wn.NOUN\n",
        "\n",
        "\n",
        "def clean_up(text):\n",
        "    \"\"\"\n",
        "    Function to clean data.\n",
        "    Steps:\n",
        "    - Removing special characters, numbers\n",
        "    - Lemmatization\n",
        "    - Stop-words removal\n",
        "    - Getting a unique list of words\n",
        "    - TODO: try removing names and company names like Navient (Proper nouns)\n",
        "    \"\"\"\n",
        "    #lemma = WordNetLemmatizer()\n",
        "    lemmatizer = nltk.WordNetLemmatizer().lemmatize\n",
        "    text = re.sub('\\W+', ' ', str(text))\n",
        "    text = re.sub(r'[0-9]+', '', text.lower())\n",
        "    # correcting spellings of words using TextBlob - user complaints are bound to have spelling mistakes\n",
        "    # However, this idea was later dropped because TextBlob may change the words.\n",
        "    # text = TextBlob(text).correct()\n",
        "    word_pos = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    normalized_text_lst = [lemmatizer(x[0], get_wordnet_pos(x[1])).lower() for x in word_pos]\n",
        "    stop_words_free = [i for i in normalized_text_lst if i not in english_stopwords and len(i) > 3]\n",
        "    stop_words_free = list(set(stop_words_free))\n",
        "    return(stop_words_free)\n",
        "\n",
        "\n",
        "def get_average_word2vec(complaints_lst, model, num_features=300):\n",
        "    \"\"\"\n",
        "    Function to average the vectors in a list.\n",
        "    Say a list contains 'flower' and 'leaf'. Then this function gives - model[flower] + model[leaf]/2\n",
        "    - index2words gets the list of words in the model.\n",
        "    - Gets the list of words that are contained in index2words (vectorized_lst) and \n",
        "      the number of those words (nwords).\n",
        "    - Gets the average using these two and numpy.\n",
        "    \"\"\"\n",
        "    #complaint_feature_vecs = np.zeros((len(complaints_lst),num_features), dtype=\"float32\") #?used?\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    vectorized_lst = []\n",
        "    vectorized_lst = [model[word] if word in index2word_set else np.zeros(num_features) for word in                       complaints_lst]    \n",
        "    nwords = len(vectorized_lst)\n",
        "    summed = np.sum(vectorized_lst, axis=0)\n",
        "    averaged_vector = np.divide(summed, nwords)\n",
        "    return averaged_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = main_df\n",
        "df = df[['Product', 'Consumer complaint narrative']]\n",
        "\n",
        "\n",
        "df = df[pd.notnull(df['Consumer complaint narrative'])]\n",
        "df = df.rename({'Consumer complaint narrative':'complaint', 'Product':'product'},\n",
        "               axis='columns')\n",
        "\n",
        "products_count_df = df.groupby('product').complaint.count().to_frame()\n",
        "products_count_df.reset_index(level=0, inplace=True)\n",
        "\n",
        "\n",
        "values = np.array([2,5,3,6,4,7,1])   \n",
        "idx = np.array(df['product']) \n",
        "clrs = ['grey' if (x < max(values)) else 'red' for x in values ]\n",
        "sb.barplot(x=idx, y=values, palette=clrs) # color=clrs)\n",
        "\n",
        "\n",
        "alt.Chart(products_count_df).mark_bar().encode(\n",
        "    x='product:O',\n",
        "    y=\"complaint:Q\",\n",
        "    color=alt.condition(\n",
        "        alt.datum.product == 'Debt collection',\n",
        "        alt.value('orange'),  \n",
        "        alt.value('steelblue')   \n",
        "    )\n",
        ").properties(width=550)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "ax = sns.barplot(products_count_df['product'], products_count_df['complaint'])\n",
        "ax.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n",
        "ax.set(xlabel=\"Product\", ylabel='Count')\n",
        "ax.set_xticklabels(products_count_df['product'])\n",
        "for item in ax.get_xticklabels(): item.set_rotation(90)\n",
        "for i, v in enumerate(products_count_df[\"complaint\"].iteritems()):        \n",
        "    ax.text(i ,v[1], \"{:,}\".format(v[1]), color='m', va ='bottom', rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# ## Preprocessing of Data\n",
        "\n",
        "# Dealing with date - Removed after these considerations:\n",
        "# 1. The model accuracy wasn't getting affected a lot. By few decimal places\n",
        "# 2. Some models don't take negative values\n",
        "# Code for handling date is commented out.\n",
        "# \n",
        "# Up-sampling would help in this case, to increase the number of instances of classes like Virtual currency and Other financial services as these have very less cases.\n",
        "\n",
        "\n",
        "# df['year'] = df['received_dt'].str.slice(6, 10).astype(int)\n",
        "# df['day'] = df['received_dt'].str.slice(3, 5).astype(int)\n",
        "# df['month'] = df['received_dt'].str.slice(0, 2).astype(int)\n",
        "\n",
        "# # calculate and check these values once -- !! --\n",
        "# df['sin_day'] = np.sin(2*np.pi*(df['day'] - 1)/np.vectorize(calc_monthrange)(df['year'], df['month']))\n",
        "# df['cos_day'] = np.cos(2*np.pi*(df['day'] - 1)/np.vectorize(calc_monthrange)(df['year'], df['month']))\n",
        "\n",
        "# df['sin_month'] = np.sin(2*np.pi*(df['month'] - 1)/12)\n",
        "# df['cos_month'] = np.cos(2*np.pi*(df['month'] - 1)/12)\n",
        "\n",
        "# drop_columns = ['received_dt', 'year', 'month', 'day', ]\n",
        "# df.drop(drop_columns, inplace=True, axis=1)\n",
        "\n",
        "# ---\n",
        "# The next two lines take some time to run because they are cleaning each row of the text.\n",
        "# I saved this dataframe and now, I'm loading the same here.\n",
        "# df['complaint'] = df['complaint'].apply(clean_up)\n",
        "# df.to_csv(\"data/modified/output_consumer_complaints.csv\", index=False)\n",
        "# ---\n",
        "\n",
        "# Loading this from the saved version of this file.\n",
        "input_df = pd.read_csv('data/modified/output_consumer_complaints.csv', \n",
        "                       converters={\"complaint\": literal_eval}) #to fetch date as a list\n",
        "input_df.head()\n",
        "\n",
        "# Some rows have been stripped of words after the cleaning process and contain empty lists\n",
        "# These have to be removed as they don't help in the prediction problem at all.\n",
        "# Moreover, they produce NaNs after going through word2Vec, giving a vector of 300 (num_of_features) NaNs each.\n",
        "print(\"Removing rows which have been stripped of words after the cleaning process. These contain empty lists:\")\n",
        "print(\"Before: \", input_df.shape)\n",
        "input_df = input_df[input_df.astype(str)['complaint'] != '[]']\n",
        "print(\"After: \", input_df.shape)\n",
        "# 14 rows removed in total.\n",
        "bow_input_df = input_df\n",
        "\n",
        "\n",
        "# ## Technique I: Bag of Words Model (CountVectorizer)\n",
        "\n",
        "min_word_count = 10\n",
        "products_count_df = bow_input_df.groupby('product').complaint.count().to_frame()\n",
        "products_count_df.reset_index(level=0, inplace=True)\n",
        "class_labels = array(products_count_df['product'].unique())\n",
        "\n",
        "\n",
        "# TfidfVectorizer handles tokenization.\n",
        "bow_input_df['complaints_untokenized'] = bow_input_df['complaint'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Goes out of memory without max_features on p2.xlarge. I tried to increase max_features but the memory error \n",
        "# wouldn't allow me.\n",
        "tfidf_converter = TfidfVectorizer(max_features=1500, sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', \n",
        "                                  stop_words='english')\n",
        "features = tfidf_converter.fit_transform(bow_input_df.complaints_untokenized).toarray()\n",
        "labels = class_labels\n",
        "# Naive Bayes takes only non-negative values. So not adding the date features to the model.\n",
        "\n",
        "\n",
        "ktrain_x, ktest_x, ktrain_y, ktest_y = train_test_split(bow_input_df['complaints_untokenized'], bow_input_df['product'], \n",
        "                                                        test_size=0.3, random_state=123)\n",
        "\n",
        "# Used later for keras.\n",
        "train_df = pd.concat([pd.DataFrame(ktrain_x), pd.DataFrame(ktrain_y, columns=[\"product\"])], axis=1)      \n",
        "val_df = train_df.sample(frac=0.2,random_state=200)\n",
        "test_df = pd.concat([pd.DataFrame(ktest_x), pd.DataFrame(ktest_y, columns=[\"product\"])], axis=1)\n",
        "\n",
        "#texts = train_df.complaint\n",
        "print(val_df.shape)\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)\n",
        "\n",
        "\n",
        "# ## Training and Test Sets\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(features, bow_input_df['product'], test_size=0.3,\n",
        "                                                    random_state=123)\n",
        "\n",
        "\n",
        "# ## Modelling\n",
        "# The TF-IDF model used only allows a limited number of words to be chosen as the features. Going beyond that gives an out-of-memory error. Given that this limit reduces the feature space, the results expected would be better with more memory power.\n",
        "# \n",
        "# Models that I implemented:\n",
        "# \n",
        "# 1. Naive Bayes: Simple and can work pretty well.\n",
        "# 2. Random Forest: It is known to perform quite well by using dumb classfiers to build a powerful learning model\n",
        "# 3. Linear SVM: Seemed to perform well on examples online.\n",
        "# \n",
        "# Given more time, I would have tried Logistic Regression, Neural nets, etc.\n",
        "\n",
        "# Model 1: Naive Bayes model\n",
        "bayes_model = MultinomialNB()\n",
        "bayes_clf = bayes_model.fit(train_x, train_y)\n",
        "\n",
        "\n",
        "naive_preds = bayes_clf.predict(test_x)\n",
        "naive_preds_conf = bayes_clf.predict_proba(test_x)\n",
        "naive_preds_conf_df = pd.DataFrame(naive_preds_conf, index=range(naive_preds_conf.shape[0]),\n",
        "                          columns=range(naive_preds_conf.shape[1]))\n",
        "naive_preds_conf_df['predicted_conf'] = naive_preds_conf_df.max(axis=1)\n",
        "naive_results = pd.DataFrame(data={\"actual_label\":test_y, \"predicted_label\":naive_preds})\n",
        "\n",
        "naive_results = pd.concat([naive_results, naive_preds_conf_df['predicted_conf']], axis=1)\n",
        "# Accuracy: wherever the labels were correctly predicted.\n",
        "naive_results['correctly_predicted'] = np.where(naive_results['actual_label'] == naive_results['predicted_label'], \n",
        "                                                1, 0)\n",
        "naive_accuracy = (naive_results['correctly_predicted'].sum()/naive_results.shape[0])*100\n",
        "print(\"Accuracy of Naive Bayes is: {0:.2f}.\".format(naive_accuracy))\n",
        "# 37.78 with 1500 features\n",
        "\n",
        "\n",
        "print(metrics.classification_report(test_y, naive_preds, target_names=bow_input_df['product'].unique()))\n",
        "\n",
        "\n",
        "# Model 2: Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators = 100, max_features='sqrt', verbose=1, warm_start=False,\n",
        "                                 random_state=123) #ntrainDataVecs=4, max_depth=2)\n",
        "rf_clf = rf_model.fit(train_x, train_y)\n",
        "rf_preds = rf_clf.predict(test_x)\n",
        "rf_preds_conf = rf_clf.predict_proba(test_x)\n",
        "rf_preds_conf_df = pd.DataFrame(rf_preds_conf, index=range(rf_preds_conf.shape[0]),\n",
        "                          columns=range(rf_preds_conf.shape[1]))\n",
        "\n",
        "rf_preds_conf_df['predicted_conf'] = rf_preds_conf_df.max(axis=1)\n",
        "rf_results = pd.DataFrame(data={\"actual_label\":test_y, \"predicted_label\":rf_preds})\n",
        "rf_results = pd.concat([rf_results, rf_preds_conf_df['predicted_conf']], axis=1)\n",
        "\n",
        "\n",
        "# Accuracy: wherever the labels were correctly predicted.\n",
        "rf_results['correctly_predicted'] = np.where(rf_results['actual_label'] == rf_results['predicted_label'], 1, 0)\n",
        "rf_accuracy = (rf_results['correctly_predicted'].sum()/rf_results.shape[0])*100\n",
        "print(\"Accuracy of Random Forest obtained is: {0:.2f}.\".format(rf_accuracy))\n",
        "print(metrics.classification_report(test_y, rf_preds, target_names=bow_input_df['product'].unique()))\n",
        "\n",
        "\n",
        "# Model 3: Linear Support Vector Machine\n",
        "svc_model = LinearSVC()\n",
        "svcc_model = CalibratedClassifierCV(svc_model)     \n",
        "svc_clf = svcc_model.fit(train_x, train_y)\n",
        "svc_preds = svcc_model.predict(test_x)\n",
        "svc_preds_conf = svcc_model.predict_proba(test_x)\n",
        "svc_preds_conf_df = pd.DataFrame(svc_preds_conf, index=range(svc_preds_conf.shape[0]), \n",
        "                                columns=range(svc_preds_conf.shape[1]))\n",
        "svc_preds_conf_df['predicted_conf'] = svc_preds_conf_df.max(axis=1)\n",
        "svc_results = pd.DataFrame(data={'actual_label':test_y, 'predicted_label':svc_preds})\n",
        "\n",
        "svc_results = pd.concat([svc_results, svc_preds_conf_df['predicted_conf']],axis=1)\n",
        "\n",
        "# Accuracy: wherever the labels were correctly predicted.\n",
        "svc_results['correctly_predicted'] = np.where(svc_results['actual_label'] == svc_results['predicted_label'], 1, 0)\n",
        "svc_results.head()\n",
        "svc_accuracy = (svc_results['correctly_predicted'].sum()/svc_results.shape[0])*100\n",
        "print(\"Accuracy of Linear SVC obtained is: {0:.2f}.\".format(svc_accuracy))\n",
        "print(metrics.classification_report(test_y, svc_preds, target_names=bow_input_df['product'].unique()))\n",
        "\n",
        "\n",
        "# #### Comparison between the models implemented\n",
        "# - Of the Bag of Words models implemented, Random Forest performs the best with an accuracy of 41.49%.\n",
        "# - As the number of features are limited in this Bag of Words model, this is expected. Sometimes, the Bag of Words model can\n",
        "#   outperform techniques like Word2Vec.\n",
        "# \n",
        "# #### Accuracies\n",
        "# - Naive Bayes - 37.03\n",
        "# - Random Forest - 41.49\n",
        "# - Linear SVC - 40.01\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# ## Technique 2: Word2Vec\n",
        "# I tried creating my own model for Word2Vec. However, this only contained 17million words, as opposed to Google's GoogleNews' pretrained Word2Vec model. So, I chose to go ahead with the pre-trained model.\n",
        "# In lieu of time, I couldn't do this - but I would have preferred to complement the Google Word2Vec model with words from this dataset. This Word2Vec model is up until 2013, post which slang/other important words might have been introduced in the vocabulary. \n",
        "# Of course, these words could also be company-complaint specific. For example, for ATB Bank, someone might be using ATB bank or a specific Policy name like ATBUltraInsurance. These would also be removed.\n",
        "# Apart from this, these complaints contain a lot of spelling mistakes and words joined together. Such as: `immeditalely`, `demaging`,  `practiciing`, etc. (shown as missing_words in the cells below), and two words joined together into one word, such as 'givenrequesting'.\n",
        "# I tried looking into it and found out about a library called TextBlob. However, people also warned against its used because it might not always be right. So I chose to not use it and skip over these words for now.\n",
        "# There were also short forms not detected by the model.\n",
        "\n",
        "\n",
        "# Creating a Word2Vec model using training set\n",
        "vocabulary_of_all_words = input_df['complaint'].tolist()\n",
        "num_features = 300\n",
        "min_word_count = 10                      \n",
        "num_workers = 8\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsampling for frequent words\n",
        "word2vec_model_name = \"trained_models/300features_10minwords_10context1\"\n",
        "word2vec_complaints = word2vec.Word2Vec(vocabulary_of_all_words, workers=num_workers, size=num_features, \n",
        "                                   min_count=min_word_count, window=context, sample=downsampling)\n",
        "word2vec_complaints.save(word2vec_model_name)\n",
        "\n",
        "# Fetching trained model to save time.\n",
        "word2vec_complaints = gensim.models.Word2Vec.load(word2vec_model_name)\n",
        "\n",
        "vocab_lst_flat = [item for sublist in vocabulary_of_all_words for item in sublist]\n",
        "vocab_lst_flat = list(set(vocab_lst_flat))\n",
        "# Loading a pre-trained GoogleNews model\n",
        "# word2vec_model = KeyedVectors.load_word2vec_format(\"trained_models/GoogleNews-vectors-negative300.bin\", binary=True)\n",
        "\n",
        "# Exploring this model to see how well it has trained and checking for spelling mistakes in user-complaints\n",
        "try:\n",
        "    word2vec_complaints.wv.most_similar(\"good\")\n",
        "except KeyError:\n",
        "    print(\"Sorry, this word doesn't exist in the vocabulary.\")\n",
        "    \n",
        "words_not_present = 0\n",
        "words_present = 0\n",
        "total_unique_tokens = len(set(vocab_lst_flat))\n",
        "missing_words = []\n",
        "for i in vocab_lst_flat:\n",
        "    try:\n",
        "        p = word2vec_complaints[i]\n",
        "        words_present+=1\n",
        "    except KeyError:\n",
        "        missing_words.append(i)\n",
        "        words_not_present+=1\n",
        "print(words_present, words_not_present, total_unique_tokens)\n",
        "\n",
        "# Examples of spelling mistakes, grammatical errors, etc.\n",
        "print(missing_words[:20])\n",
        "\n",
        "\n",
        "# #### Choosing a Word2Vec Model\n",
        "# - The Google word2vec model isn't able to account for a lot of words. It can be made better by retraining on more words from the training set. However, a lot of these words are spelling mistakes.\n",
        "# - The presence of 'xxxx', 'xx', etc. in various forms is a simple fix which can also be implemented.\n",
        "# - Initially, I had planned to use Google's pretrained Word2Vec model. However, after waiting for hours for training on Google word2vec model, I switched back to the Word2Vec model for want of speed.\n",
        "\n",
        "\n",
        "# # These take a very long time to be averaged. Commenting this code and reading from file the saved output.\n",
        "# embeddings_df = input_df['complaint'].apply(lambda complaint: get_average_word2vec(complaint, word2vec_complaints, \n",
        "#                                                                                    num_features)).to_frame()\n",
        "# col_lst = []\n",
        "# for i in range(num_features):\n",
        "#     col_lst.append('vec_'+str(i+1))\n",
        "# # Easy to write to file and process when exploded into columns\n",
        "# exploded_em_df = pd.DataFrame(embeddings_df.complaint.tolist(), columns=col_lst)\n",
        "# exploded_em_df = pd.DataFrame(embeddings_df)['complaint'].apply(pd.Series)\n",
        "# exploded_em_df.head()\n",
        "# exploded_em_df.to_csv(\"data/modified/vocab_trained_word2Vec.csv\", index=False)\n",
        "\n",
        "exploded_em_df = pd.read_csv('data/modified/vocab_trained_word2Vec.csv')\n",
        "print(\"Word2Vec output:\\n\")\n",
        "exploded_em_df.head()\n",
        "\n",
        "input_df = input_df.reset_index(drop=True)\n",
        "vectorized_df = pd.concat([exploded_em_df, input_df[['product']]], axis=1)                        \n",
        "vectorized_df = shuffle(vectorized_df)\n",
        "\n",
        "if vectorized_df[vectorized_df.isnull().any(axis=1)].empty:\n",
        "    res = \"True\" # No NaNs exist in the cleaned dataset.\n",
        "else:\n",
        "    res = \"False\"\n",
        "print(res)\n",
        "print(vectorized_df.shape)\n",
        "if not res:\n",
        "    vectorized_df[vectorized_df.isnull().any(axis=1)]\n",
        "    vectorized_df.dropna(axis=0, how='any')\n",
        "    print(vectorized_df.shape)\n",
        "\n",
        "\n",
        "# ### Training and Test Sets]\n",
        "\n",
        "vectorized_data = np.array(vectorized_df.drop('product', axis=1))\n",
        "vectorized_target = np.array(vectorized_df['product'])\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(vectorized_data, vectorized_target,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=123)\n",
        "\n",
        "\n",
        "# ### Modelling\n",
        "# Some of the models that I tried:\n",
        "# 1. Random Forest Classifier: It is known to perform quite well by using dumb classfiers to build a powerful learning model\n",
        "# 2. Linear SVC: Seemed to perform well on examples online.\n",
        "# 3. Deep Neural Network - CNN: Upon reading online some discussion on this, I thought of implementing CNNs. It said - what has recently been shown to work much better and simpler than RNNs is using word vectors, pre-trained on a large corpus, as features to the neural network. RNNs were called 'slow and fickle to train'.\n",
        "\n",
        "# Model 1: Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(n_estimators = 100, max_features='sqrt', verbose=1, warm_start=False,\n",
        "                                 random_state=123) #ntrainDataVecs=4, max_depth=2)\n",
        "rf_clf = rf_model.fit(train_x, train_y)\n",
        "rf_preds = rf_clf.predict(test_x)\n",
        "rf_preds_conf = rf_clf.predict_proba(test_x)\n",
        "\n",
        "rf_preds_conf_df = pd.DataFrame(rf_preds_conf, index=range(rf_preds_conf.shape[0]),\n",
        "                          columns=range(rf_preds_conf.shape[1]))\n",
        "rf_preds_conf_df['predicted_conf'] = rf_preds_conf_df.max(axis=1)\n",
        "rf_results = pd.DataFrame(data={\"actual_label\":test_y, \"predicted_label\":rf_preds})\n",
        "\n",
        "rf_results = pd.concat([rf_results, rf_preds_conf_df['predicted_conf']],\n",
        "                          axis=1)\n",
        "# Accuracy: wherever the labels were correctly predicted.\n",
        "rf_results['correctly_predicted'] = np.where(rf_results['actual_label'] == rf_results['predicted_label'], 1, 0)\n",
        "rf_accuracy = (rf_results['correctly_predicted'].sum()/rf_results.shape[0])*100\n",
        "\n",
        "# Accuracy: wherever the labels were correctly predicted.\n",
        "rf_results['correctly_predicted'] = np.where(rf_results['actual_label'] == rf_results['predicted_label'], 1, 0)\n",
        "rf_accuracy = (rf_results['correctly_predicted'].sum()/rf_results.shape[0])*100\n",
        "print(\"Accuracy of Random Forest obtained is: {0:.2f}.\".format(rf_accuracy))\n",
        "print(metrics.classification_report(test_y, rf_preds, target_names=bow_input_df['product'].unique()))\n",
        "\n",
        "\n",
        "# Model 2: Linear Support Vector Machine\n",
        "svc_model = LinearSVC()\n",
        "# CV = 5\n",
        "# accuracy = cross_val_score(svc_model, train_x, train_y, scoring='accuracy', cv=CV)\n",
        "# accuracy.mean() - obtained was 64.29%\n",
        "svcc_model = CalibratedClassifierCV(svc_model)     \n",
        "svc_clf = svcc_model.fit(train_x, train_y)\n",
        "\n",
        "svc_preds = svcc_model.predict(test_x)\n",
        "svc_preds_conf = svcc_model.predict_proba(test_x)\n",
        "svc_preds_conf_df = pd.DataFrame(svc_preds_conf, index=range(svc_preds_conf.shape[0]), \n",
        "                                columns=range(svc_preds_conf.shape[1]))\n",
        "svc_preds_conf_df['predicted_conf'] = svc_preds_conf_df.max(axis=1)\n",
        "svc_results = pd.DataFrame(data={'actual_label':test_y, 'predicted_label':svc_preds})\n",
        "\n",
        "svc_results = pd.concat([svc_results, svc_preds_conf_df['predicted_conf']],axis=1)\n",
        "# Accuracy: wherever the labels were correctly predicted.\n",
        "svc_results['correctly_predicted'] = np.where(svc_results['actual_label'] == svc_results['predicted_label'], 1, 0)\n",
        "svc_results.head()\n",
        "svc_accuracy = (svc_results['correctly_predicted'].sum()/svc_results.shape[0])*100\n",
        "\n",
        "# Accuracy: wherever the labels were correctly predicted.\n",
        "svc_results['correctly_predicted'] = np.where(svc_results['actual_label'] == svc_results['predicted_label'], 1, 0)\n",
        "svc_results.head()\n",
        "svc_accuracy = (svc_results['correctly_predicted'].sum()/svc_results.shape[0])*100\n",
        "print(\"Accuracy of Linear SVC obtained is: {0:.2f}.\".format(svc_accuracy))\n",
        "print(metrics.classification_report(test_y, svc_preds, target_names=bow_input_df['product'].unique()))\n",
        "\n",
        "\n",
        "# - These models are performing much better than Bag of Words model for my data.\n",
        "# - I haven't been able to run this model properly. Because I ran it on a subset of values, class imbalance was causing the code to fail because y_train was not getting the right shape - it didn't have all possible value types. I would like to train this model with the complete code to check the results.\n",
        "# - The accuracies obtained with Random Forest is 65.30%, 65.16% with Linear SVC. These models have better accuracies than Bag of Words model and of these Random Forest performs the best with vector embeddings as the input.\n",
        "#\n",
        "\n",
        "### Evaluation using the best Model: Random Forest using vector embeddings\n",
        "#Confusion Matrix\n",
        "products_count_df = df.groupby('product').complaint.count().to_frame()\n",
        "products_count_df.reset_index(level=0, inplace=True)\n",
        "product_class_labels = array(products_count_df['product'].unique())\n",
        "\n",
        "conf_mat = confusion_matrix(test_y, rf_preds)\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=product_class_labels, yticklabels=product_class_labels)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# - Lot of values can be seen on the diagonal, which is good for the model. Upsampling would probably help our case.\n",
        "# - I also implemented a CNN Model with Keras using Keras Tokenizer. This isn't completely done yet. I'm still mentioning the code for your review.\n",
        "\n",
        "\n",
        "# Model 3: CNN using Keras\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "NUM_WORDS = 20000\n",
        "texts = train_df.complaints_untokenized\n",
        "products_unique = vectorized_df['product'].unique()\n",
        "\n",
        "dict_products = {}\n",
        "for i, complaint in enumerate(products_unique):\n",
        "    dict_products[complaint] = i\n",
        "labels = vectorized_df['product'].apply(lambda x:dict_products[x])\n",
        "\n",
        "vocab_lst_flat = [item for sublist in vocabulary_of_all_words for item in sublist]\n",
        "\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',\n",
        "                      lower=True)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences_train = tokenizer.texts_to_sequences(texts)\n",
        "sequences_valid=tokenizer.texts_to_sequences(val_df.complaints_untokenized)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "EMBEDDING_DIM=300\n",
        "vocabulary_size=min(len(word_index) + 1, NUM_WORDS)\n",
        "embedding_layer = Embedding(vocabulary_size, EMBEDDING_DIM)\n",
        "\n",
        "    \n",
        "train_df = train_df.drop(val_df.index)\n",
        "                    \n",
        "size_train = len(train_x)\n",
        "size_test = len(test_x)\n",
        "output_labels_unique = np.asarray(sorted(list(set(labels))))\n",
        "\n",
        "X_train = pad_sequences(sequences_train)\n",
        "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1]) #test\n",
        "# convert into dummy representation of the output labels\n",
        "y_train = to_categorical(np.asarray(labels[train_df.index]))\n",
        "y_val = to_categorical(np.asarray(labels[val_df.index]))\n",
        "\n",
        "sequence_length = X_train.shape[1]\n",
        "filter_sizes = [3,4,5]\n",
        "num_filters = 100\n",
        "drop = 0.5\n",
        "\n",
        "output_dim = len(products_unique)\n",
        "\n",
        "print('Shape of X train and X test tensors:', X_train.shape, X_val.shape)\n",
        "print('Shape of label train and test tensors:', y_train.shape, y_val.shape)\n",
        "\n",
        "inputs = Input(shape=(sequence_length,))\n",
        "embedding = embedding_layer(inputs)\n",
        "reshape = Reshape((sequence_length, EMBEDDING_DIM, 1))(embedding)\n",
        "\n",
        "conv_0 = Conv2D(num_filters, (filter_sizes[0], EMBEDDING_DIM), activation='relu', \n",
        "                                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_1 = Conv2D(num_filters, (filter_sizes[1], EMBEDDING_DIM), activation='relu', \n",
        "                                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "conv_2 = Conv2D(num_filters, (filter_sizes[2], EMBEDDING_DIM), activation='relu', \n",
        "                                kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
        "\n",
        "maxpool_0 = MaxPooling2D((sequence_length - filter_sizes[0] + 1, 1), strides=(1,1))(conv_0)\n",
        "maxpool_1 = MaxPooling2D((sequence_length - filter_sizes[1] + 1, 1), strides=(1,1))(conv_1)\n",
        "maxpool_2 = MaxPooling2D((sequence_length - filter_sizes[2] + 1, 1), strides=(1,1))(conv_2)\n",
        "\n",
        "merged_tensor = concatenate([maxpool_0, maxpool_1, maxpool_2], axis=1)\n",
        "flatten = Flatten()(merged_tensor)\n",
        "reshape = Reshape((3*num_filters,))(flatten)\n",
        "dropout = Dropout(drop)(flatten)\n",
        "output = Dense(units=output_dim, activation='softmax', kernel_regularizer=regularizers.l2(0.01))(dropout)\n",
        "\n",
        "cnn_model = Model(inputs, output)\n",
        "adam = Adam(lr=1e-3)\n",
        "cnn_model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=['acc'])\n",
        "callbacks = [EarlyStopping(monitor='val_loss')]\n",
        "\n",
        "cnn_model.fit(X_train, y_train, batch_size=1000, epochs=10, verbose=1, validation_data=(X_val, y_val),\n",
        "                callbacks=callbacks)\n",
        "\n",
        "# Predicting on the test set\n",
        "sequences_test = test_x\n",
        "X_test = pad_sequences(sequences_test, maxlen=X_train.shape[1])\n",
        "cnn_preds = cnn_model.predict(X_test)\n",
        "print(\"Predictions from CNN completed.\")\n",
        "\n",
        "\n",
        "cnn_results = pd.DataFrame(data={\"actual_label\":test_y, \"predicted_label\":cnn_preds})\n",
        "# Accuracy: wherever the labels were correctly predicted.\n",
        "cnn_results['correctly_predicted'] = np.where(cnn_results['actual_label'] == cnn_results['predicted_label'], \n",
        "                                                1, 0)\n",
        "cnn_accuracy = (naive_results['correctly_predicted'].sum()/cnn_results.shape[0])*100\n",
        "print(\"Accuracy of the CNN Model is: {0:.2f}.\".format(cnn_accuracy))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# ## Conclusion\n",
        "# \n",
        "# - The model that performed best was: RF with word2Vec feature embeddings. It gave an accuracy measure of: 65.30%. This was obtained with the word2Vec model made out of the the training set.\n",
        "# - Due to time constraints and even after taking a p2.xlarge instance, I wasn't able to run the code to completion for Random Forest with pre-trained word2Vec model of GoogleNews.\n",
        "# - The model was saved using:\n",
        "# \n",
        "#         `pickle.dump(classifier, open('trained_models/best_model.sav', 'wb'))`\n",
        "# \n",
        "#  to save the Random Forest classifier to storage for predictions with Flask. Thereafter, it was used to make predictions.\n",
        "# \n",
        "# - I was able to derive the probabilities associated with these predictions as well. These have been displayed in the Flask application. \n",
        "# - Given more resources such as GPU, I would have run this on the complete dataset using the pre-trained Google word2Vec + CNN, which I haven't been able to.\n",
        "\n",
        "# -----------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}